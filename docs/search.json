[
  {
    "objectID": "BKP/01-intro.html",
    "href": "BKP/01-intro.html",
    "title": "Welcome to Nimbus VM",
    "section": "",
    "text": "What is a Nimbus VM?\nWhat is an ‘Artemis’..?\nThis episode introduces the Sydney Informatics Hub, Artemis HPC and how to get connected.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/01-intro.html#why-use-artemis",
    "href": "BKP/01-intro.html#why-use-artemis",
    "title": "Welcome to Nimbus VM",
    "section": "Why use Artemis?",
    "text": "Why use Artemis?\nArtemis is ideal for calculations that require: * A long time to complete (long walltime) * High RAM usage * Big data input or outputs * Are able to use multiple cores or nodes to run in parallel, and hence much faster\nArtemis is available free of charge to all University of Sydney researchers. You do need a unikey, and a valid Research Dashboard Research Data Management Plan with Artemis access enabled.\nArtemis is also a great incentive to funding bodies to view your projects favourably – as they know you have the resources required to get the work done.\nFinally, if you do use Artemis for your research, please acknowledge us! This ensures that we continue to get the funding we need to provide you with what is really a first-grade computing resource. And don’t forget to apply to the Publication Incentive! A suggested acknowledgment might say:\n\nThe authors acknowledge the Sydney Informatics Hub and the University of Sydney’s high performance computing cluster, Artemis, for providing the computing resources that have contributed to the results reported herein."
  },
  {
    "objectID": "BKP/01-intro.html#connecting-via-ssh-in-a-terminal-recommended",
    "href": "BKP/01-intro.html#connecting-via-ssh-in-a-terminal-recommended",
    "title": "Welcome to Nimbus VM",
    "section": "Connecting via SSH in a terminal (recommended)",
    "text": "Connecting via SSH in a terminal (recommended)\nDepending on your computer’s operating system, there may be several ways to connect to Artemis. The simplest way is to open your terminal emulator application, and ‘ssh’ into the Artemis login-servers. This is our recommended method, as to use Artemis effectively you should get comfortable working on the command line.\nLinux and Mac both have native terminal apps, so you only need to open them. You may also have installed one on your Windows machine.1 Go ahead and do that now. The last line displayed in your terminal window should have some information about your computer’s name, and you user name, followed by a **\\(** symbol. This is the **command prompt** -- you type your commands after the '\\)’.\n\n\n\nAn iTerm2 terminal window on Mac\n\n\nTo connect to Artemis securely, we’ll use the SSH (Secure Socket Shell) protocol; on most systems, any installed SSH client will be invoked by the command ‘ssh’. Before you connect, make sure you know your username and password. When you use Artemis for your research, these will be your Unikey and Unikey password; however, for this training course we’ll be using training accounts, which are:\n\nUsername: ict_hpctrain<N>, with N from 1-20 (replace <N> with your assigned number)\nPassword: will be written on the whiteboard!\n\nAt your command prompt, execute the following (type it and press ‘return/enter’):\nssh -X ict_hpctrain<N>@hpc.sydney.edu.au\nor, if using XQuartz on a Mac\nssh -Y ict_hpctrain<N>@hpc.sydney.edu.au\nThe -X or -Y flags tell ssh to enable X-forwarding, which lets GUI programs on Artemis serve you graphical windows back on your local machine.\nIf connecting for the first time, you may get the following output, requesting authorisation to connect to a new host server:\n``sh The authenticity of host ‘hpc.sydney.edu.au (10.250.96.203)’ can’t be established. RSA key fingerprint is SHA256:qq9FPWBcyvvOWOMdFs8uZES0tF3SVzJsNx1cdn56GSE. Are you sure you want to continue connecting (yes/no)? ```\nEnter ‘yes’. You will then be asked for your password; type it and press ‘enter’. You should then be logged in!\n\n\n\nAccess granted!"
  },
  {
    "objectID": "BKP/01-intro.html#connecting-via-an-ssh-gui-common-for-windows-users",
    "href": "BKP/01-intro.html#connecting-via-an-ssh-gui-common-for-windows-users",
    "title": "Welcome to Nimbus VM",
    "section": "Connecting via an SSH GUI (common for Windows users)",
    "text": "Connecting via an SSH GUI (common for Windows users)\nIf you’re on Windows, and followed the Setup guide, then you will likely be connecting through an X-window or shell client program, like ‘X-Win32’ or ‘PuTTY’. Following the instructions in the Setup guide:\n\nOpen your installed program\nSelect the “Artemis” session you configured earlier\nClick ‘Launch’ (X-Win32) or ‘Open’ (PuTTY)\n\nIf this is the first time connecting to Artemis, you will be asked to authorise it as a trusted host server; click ‘Accept’ (X-Win32) or ‘Yes’ (PuTTY).\n\n\n\nUnknown host challenges: X-Win32\n\n\n\n\n\nUnknown host challenges: PuTTY\n\n\n\nIf using ‘X-Win32’, you’ll then be asked for your password and once entered, you should be logged on to Artemis! A terminal window and command prompt on Artemis will appear.\nIf using ‘PuTTY’, a terminal window will appear and prompt you for your username, and then your password. Once entered, you should be logged on to Artemis! A command prompt on Artemis will appear in that window.\n\n\n\n\nAccess granted! X-Win32\n\n\n\n\n\nAccess granted! PuTTY"
  },
  {
    "objectID": "BKP/01-intro.html#connecting-via-the-graphical-login-nodes-advanced-users",
    "href": "BKP/01-intro.html#connecting-via-the-graphical-login-nodes-advanced-users",
    "title": "Welcome to Nimbus VM",
    "section": "Connecting via the Graphical Login Nodes (advanced users)",
    "text": "Connecting via the Graphical Login Nodes (advanced users)\nFor some users, it is occasionally necessary to have more reliable graphical access to the Artemis login nodes, in order to check intermediate results when using software with graphical outputs. Setup instructions are provided on the Setup page.\n\nNotes\n1↩︎ Such as ‘Cygwin’, ‘MinGW’, or even the very handy ‘Git for Windows’.\n\n\nKey points\n\nConnecting to Artemis requires a terminal emulator, and an account with access.\nUsers connect to Artemis’ login nodes only.\nOn Windows, use X-Win32, PuTTY, or another shell and terminal application of your choice.\nGUI login access is also available."
  },
  {
    "objectID": "BKP/02-navigating-artemis.html",
    "href": "BKP/02-navigating-artemis.html",
    "title": "Navigating Artemis",
    "section": "",
    "text": "Have a command you don’t understand?\nCheck out this hand tool https://explainshell.com/ to help breakdown the unix jargon.\n{: .callout}\n\n\nThe Artemis directory structure\n ## When you first arrive\nAt the end of last lesson we logged on to Artemis, and were presented with a command prompt in a terminal window. The last line in that window would look something like this:\n\n\n\nCommand prompt on Artemis, using X-Win32\n\n\n\nThe command prompt, which is where in the terminal you type your commands, ends with a $ dollar sign. Before that is some text, in the form of [username @ host current_directory ]. In the screencap above, the username is ict_hpctrain1, the host server that we have logged into is ‘login3’ (the 3rd Artemis login node), and our current cirectory is ~, which is a shortcode for the user’s ‘home’ directory.\nEven though no files are visible or listed in the terminal window, the command prompt is always ‘located’ at a specific place in the filesystem – we can print that location to the window with the pwd (print working directory) command. Type it and press ‘enter’:\n[ict_hpctrain1@login3 ~]$ pwd\n{: .language-bash}\nThe output should look something like this (if your username is ict_hpctrain1):\n/home/ict_hpctrain1\n{: .output}\nThis is your home directory. Every user on Artemis (and any Linux system) has a home directory located at /home/<username>. Your username may be your Unikey, or it may be the training accounts we’re using today. Every time you log in to Artemis, this is where you’ll end up: home. :blush:\n > ## Directions for the student > > In these courses, purple coloured block-quotes as above indicate commands that you need to enter into your command prompt. > > Black coloured block-quotes indicate output you should expect as a result.) {: .callout}\n\n\nThe three branches of the Artemis tree\n\n\n\n\nArtemis filesystem structure\n\n\n\nThe Artemis filesystem has three main branches: /home, /project, and /scratch. We have just met /home. The root folder of the filesystem is signified by a single forward-slash /; and so every other location, which is a subfolder of the root, must also begin with this slash. This it why ‘home’ is located at /home. (If this is all new to you, then you probably haven’t taken the Intro to Unix prerequisite course. You should do that ASAP).\n\ni. /home\n/home is where your data is. But not your project data, just your own personal data. If you have any project-related data that needs to remain private to you alone then also keep that in /home, as only you can see or read files in your home directory. Home directories are allocated 10 GB of storage only.\n\n\niii. /project\nThe /project branch is where researchers on a project should keep all the data and programs or scripts that are currently being used. We say ‘currently’ because project directories are allocated only 1 TB of storage space – that may sound like a lot, but this space is shared between all users of your project, and it runs out faster than you think. Data that you’re not currently working on should be deleted or moved back to its permanent storage location (which should be the Research Data Store1).\nThis also means that everyone working on your project can see all files in the project directory by default.2\nProject directories are all subfolders of /project, and will have names like\n/project/RDS-CORE-Training-RW\n{: .output}\nwhich take the form /RDS-<Faculty>-<Short_name>-RW. In the case above, ‘Training’ is the project name and ‘CORE’ stands for Core Research Facilities.\nIf you forget what your projects are called, you can always check to see which user groups (which include projects) you are a member of, using the groups command:\n[ict_hpctrain1@login3 ~]$ groups\n{: .language-bash}\n[ict_hpctrain1@login1 ~]$ groups\nlinuxusers HPC-ACL RDS-ICT-HPCTRAINING-RW RDN-USERS RDN-CORE-Training RDS-CORE-Training-RW RDN-CORE-SIHsandbox RDS-CORE-SIHsandbox-RW\n{: .output}\nNote that this user ict_hpctrain1 is a member of the RDS-ICT-HPCTRAINING-RW and RDS-CORE-Training-RW and RDS-CORE-SIHsandbox-RW projects.\nThe middle part of the project name is also called the short name, and was chosen by whomever applied for the RDMP (Research Data Management Plan) that defines the project. The full folder name /RDS-<Faculty>-<Short_name>-RW is technically the actual project name as known to Artemis, however the short names can be used throughout Artemis instead.\nSo, for example, change into the project directory of the ‘Training’ project with the change directory command cd:\n[ict_hpctrain1@login3 ~]$ cd /project/Training\n[ict_hpctrain1@login1 Training]$ pwd\n{: .language-bash}\n/project/Training\n{: .output}\n\n\niii. /scratch\nEvery project also has a /scratch directory, at /scratch/<Short_name>. /scratch is where you should actually perform your computations. What does this mean? If your workflow: * Has big data inputs * Generates big data outputs * Generates lots of intermediate data files which are not needed afterward\nthen you should put and save this data in your /scratch space. The reasons are that you are not limited to 1 TB of space in /scratch as you are in /project, and there is also no reason to clutter up your shared project directories with temporary files. Once your computation is complete, simply copy the important or re-used inputs and outputs back to your /project folder, and delete your data in /scratch – that’s why it’s called scratch!\n > ## Your data should not be kept on Artemis! > > Wait, what? Your data, long-term, should not be stored on Artemis, neither in /project nor /scratch. Artemis is not backed-up, and has limited space. >\n\n\n\nInactive data in /project is wiped after 6 months, and in /scratch after 3 months!\n\n\nData that you are currently working with should be transferred into your /project folder (or /scratch if it is very large!), and then transferred back to it’s permanent location when you’re done with that part of your investigation. For more, see the Data transfer and RDS for HPC course.\n\n\n\n{: .callout} \n\n\nChecking your disk usage\nArtemis provides a handy tool to quickly see how much disk space is currently being used by all of the projects you are a member of, and your personal /home directory. That tool is pquota:\nict_hpctrain1@login3 ~]$ pquota\n{: .language-bash}\nDisk quotas for user jdar4135 (uid 572557):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n          /home  4.973G     10G     11G       -    1505       0       0       -\nDisk quotas for group RDS-CORE-SIHclassic-RW (gid 16700):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /project    240k      1T  1.074T       -      47       0       0       -\nDisk quotas for group RDS-CORE-CLC-RW (gid 22099):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /project      4k      1T  1.074T       -       1       0       0       -\nDisk quotas for group RDS-CORE-SIHsandbox-RW (gid 16198):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /project  233.9G      1T  1.074T       -   81018       0       0       -\nDisk quotas for group RDS-CORE-Training-RW (gid 14206):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /project  31.89G      1T  1.074T       -    1746       0       0       -\nDisk quotas for group RDS-CORE-ICT-RW (gid 15839):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /project  514.3M      1T  1.074T       -      17       0       0       -\n{: .output}\nThe output from pquota helpfully lists the disk quotas allowed for each of the directories you can write to, as well as the number of files you have in that filesystem. There is no limit on the number of files allowed. The usages in /project comprise the files of all members of that project, not just your own. /scratch usage is not reported.\nTo see the disk usage of a project’s /scratch directory (or any directory), you can use the Unix ‘used disk space’ command du:\ndu -sh /scratch/Training/\n{: .language-bash}\n244M    /scratch/Training/\n{: .output}\nOr, if you’d like to know how much space members of a project have used in a directory, you can query the filesystem using its built-in utility lfs:\nlfs quota -hg RDS-CORE-Training-RW /scratch\n{: .language-bash}\nDisk quotas for group RDS-CORE-Training-RW (gid 14206):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n       /scratch  243.5M      0k      0k       -       2       0       0       -\n{: .output}\nThe amount of free /scratch space can be reported via the ‘free disk space’ command df:\ndf -h /scratch\n{: .language-bash}\nFilesystem            Size  Used Avail Use% Mounted on\n192.168.69.211@o2ib0:192.168.69.212@o2ib0:/Scratch\n                      379T  350T  9.3T  98% /scratch\n{: .output}\nNote that in the above commands the flag -h requests ‘human-readable’ file sizes (in B, M, G, T, etc), and the flag -s summarised the du output into one total count, rather than a number for every sub-directory.\n ## Software on Artemis\n\n\n\nExisting software\nA list of programs currently installed on Artemis can be found in the Artemis User Guide. Software is not ‘enabled’ on Artemis by default, and must be loaded before use. This is to avoid conflicts between different software and different versions of software, which may each be dependent on specific versions of other software which will also need to be loaded.\nThis is all handled by the modules package. You can list available programs on Artemis using the module avail command\n[ict_hpctrain1@login3 ~]$ module avail\n{: .source}\n[ict_hpctrain1@login3 ~]$ module avail\n--------------------------- /usr/local/Modules/versions ---------------------------\n3.2.10\n\n---------------------- /usr/local/Modules/3.2.10/modulefiles ----------------------\ndot         module-git  module-info modules     null        use.own\n-------------------------- /usr/local/Modules/modulefiles -------------------------\nabaqus/2016(default)    glew/2.1.0(default)     openmpi-gcc/3.0.0\nabaqus/6.14-1           glib/2.34.0(default)    openmpi-gcc/3.0.0-64\n...\ngit/2.14.1(default)     openmpi-gcc/1.8.4-6.x   zlib/1.2.8(default)\ngit/2.16.2              openmpi-gcc/1.8.4-backup\ngl2ps/1.4.0(default)    openmpi-gcc/2.1.2\n{: .output}\nThis command may take a minute to run, as there is a lot of software installed! If you know roughly what the program you want is called, you can refine the search with a keyword as extra argument, eg:\n[ict_hpctrain1@login3 ~]$ module avail mat\n{: .source}\n-------------------------- /usr/local/Modules/modulefiles --------------------------\nmatam/1.0(default)          matlab/R2013a             matlab/R2015b     matlab/R2017a     matlab/R2018a\nmathematica/11.1.1(default) matlab/R2014b(default)    matlab/R2016b     matlab/R2017b\n{: .output}\nNote that multiple versions of programs may be installed, and one will be designated default – that is the version that will be loaded if you don’t specify a version.\nEg, load a program with the module load command and its name:\n[ict_hpctrain1@login3 ~]$ module load matlab\n{: .source}\nthen list currently loaded programs with module list:\n[ict_hpctrain1@login3 ~]$ module list\n{: .source}\nCurrently Loaded Modulefiles:\n  1) matlab/R2014b\n{: .output}\nThe version matlab/R2014b was loaded, as it is the current default. Unload a loaded module with module unload and its name. For more info, check out the module manual page, by executing man module.\n > ## Don’t get burned by a version mismatch! > > Software usage, syntax and behaviour sometimes change between versions, and if you don’t specify a version you may soon find that your code no longer runs or, much worse, it does run, but with different results! > > For the sake of your research, and scientific reproducibility, you should always specify the version of the module you are loading in your scripts, and record it in your notes.\n{: .callout} \n\n\nInstalling new software\nAll software on Artemis is stored in /usr/local. Users do not have persmission to write to this folder, and hence cannot install new software. If you require a particular piece of software on Artemis, you can submit a High Performance Computing Request through the ICT Service Portal.\nAlternatively, you may be able to install Linux software directly into your userspace, ie in your /home directory. Not all software can be installed in this way, and there may also be licensing issues – so don’t try this unless you know what you’re doing, and please contact us if you don’t!\n ## Writing script files\n\n\nText editors\nArtemis has a number of text editors available for use, and of course you could install your own. Text editors are simple-to-sophisticated programs that let you, quite simply, write text! At the most basic level, they do not have any formatting, like bold etc, only ‘plain’ text. Many allow the composition of ‘rich text’, which does add formatting.3 However, our purpose will be to write scripts that other programs will read in and execute. These programs won’t be looking at formatting, but will be parsing what we write according to the syntax and keywords of the programming language in use.\nTo aid with this, some text editors feature syntax highlighting, which involves formatting text differently depending on what function that text performs in a given programming language, for example the MATLAB code below:\nfunction hello_world()\n% A function to say hello!\n  disp('Hello, World!')\nend\n{: .language-matlab}\n\n\ni. nano (recommended)\n‘Nano’ is a basic, text-only editor with very few other features, and runs inside a terminal window. This makes it fast and simple to use, but it may take some getting used to for those unfamiliar with command-line programs. There is no ‘point-and-click’ interface to nano, only special key combinations to send commands to the program. Eg, to save a file in nano, you would hit the hotkey for ‘WriteOut’: Ctrl+o.\nTo open nano, simply execute nano at the command prompt:\n[jdar4135@login2 ~]$ nano\n{: .language-bash}\n\n\n\nThe simple, text-only nano text editor. Note the hotkey command list at bottom\n\n\n\n\n\nii. gedit\nFor those who’d prefer to use a GUI (graphical user interface) with mouse support, ‘gedit’ is good option, and very user friendly. It also performs syntax highlighting, which can be activated from the View menu: View > Highlight Mode, and then select the language you would like to parse.\nTo use gedit on Artemis, you will need to have X-Forwarding enabled (see Setup guide), which means you’ll need to be either on Linux, Mac, or using ‘X-Win32’ on Windows.4\nOpen gedit by executing gedit &; the & character (called an ‘ampersand’) tells the shell to open the process running gedit in the background, allowing you to continue using your terminal while gedit remains open.\n[jdar4135@login2 ~]$ gedit &\n{: .language-bash}\n\n\n\nA gedit window, with syntax highlighting, served by XQuartz on a Mac\n\n\n\n Common operations in nano and gedit\n\n\n\n\n\n\n\n\nOperation\nin nano\nin gedit\n\n\n\n\nopen new file\nnano\ngedit &\n\n\nopen new file ‘NewFile’\nnano NewFile\ngedit NewFile &\n\n\nopen existing file\nnano ExistingFile\ngedit ExistingFile &\n\n\nsave\nCtrl+o\nCtrl+s\n\n\nexit\nCtrl+x\nCtrl+q\n\n\n\n ### iii. others\nThere are dozens of text editors. Some others that you may be familiar with include ‘Emacs’ and ‘vi/vim’. These are available on Artemis, and Emacs can be used in GUI mode as well.\n Common text editors\n\n\n\nEditor\nEase of use\nPower/flexibility\nGUI\n\n\n\n\nnano\nModerate to High\nLow\nNo\n\n\ngedit\nHigh\nLow\nYes\n\n\nEmacs\nHigh (GUI), Moderate\nHigh\nYes\n\n\nvi/vim\nLow\nHigh\nNo\n\n\n\n > ## Windows character encoding errors > > Windows and Linux/OSX use different character encoding. This means that some characters (or invisibles!) that appear the same are actually encoded differently – leading to obscure errors! > > This will arise when writing code on Windows machines to be used on Linux ones (like Artemis). > > Common encoding issues include: >* dashes of different lengths, eg - vs – vs —. This is often a problem when you copy and paste from PDFs onto the command line. >* line endings. Some editors like ‘Notepad++’ on Windows let you set which kind of line endings to encode with. Select ‘Unix (LF)’ if you have the option. {: .callout}\n\n\nNotes\n1↩︎ The Research Data Store (RDS) is covered in the 2nd lesson of this series, Introduction to the Research Data Store and Data Transfer.\n2↩︎ There are ways to change the user permissions of files and folders in Linux, but we won’t cover that here. Don’t try it unless you know what you’re doing!\n3↩︎ Programs such as Microsoft Word are not really text-editors, but more ‘word processors’, in that they do a lot more than simply compose text. There is not a hard line between the two categories.\n4↩︎ If you prefer to use PuTTY terminal, you can use GUI text editors by having both PuTTY and X-Win32 installed. Having X-Win32 open acts as the X server for PuTTY.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/03-pbs-scripts.html",
    "href": "BKP/03-pbs-scripts.html",
    "title": "Writing PBS submission scripts",
    "section": "",
    "text": "This episode introduces the scheduler and how to communicate with it. Primarily, this will be through ‘PBS directives’ in Bash scripts.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/03-pbs-scripts.html#the-scheduler",
    "href": "BKP/03-pbs-scripts.html#the-scheduler",
    "title": "Writing PBS submission scripts",
    "section": "The ‘scheduler’",
    "text": "The ‘scheduler’\nArtemis HPC is a computer cluster – a network – whose resources are assigned to users by a scheduler. The cluster’s resources are the CPUs, RAM and GPUs which belong to all the constituent computers in the network. Curently, Artemis has:\n\n7,636 cores (CPUs)\n45 TB of RAM\n108 NVIDIA V100 GPUs\n378 TB of storage\n\nThese resources are distributed between all of its computers, which we sometimes call ‘machines’, and often refer to as nodes. It’s important to realise that these are real computers, and each node actually has a set number of CPUs and RAM etc that form it – it’s not all one huge computer.\nIn order to make the most efficient use of this network, a very complicated algorithm tries to allocate nodes and their resources to different users who want them, with the twin aims of having no resources sitting idle and as little waiting time as possible for the users. Clearly, these aims are in conflict!\n\n\n\nA cartoon scheduler – compute jobs will try to be resourced as efficiently as possible!\n\n\n\nThe scheduler does these computations. Artemis’ scheduler is a version of ‘PBS Pro’. The scheduler maintains a queue of computing jobs that users have submitted to be run. It constantly recomputes the best order in which to run them all, based on what each job needs, what resources are available, and how long a job has been waiting for. Spare a thought for the much put-upon scheduler!\n\nCluster resources\nThe primary cluster resources available to compute jobs are:\n\nncpus: the number of CPU cores\nmem: the amount of RAM\nngpus: the number of GPU cores\nwalltime: the length of time all these resources will be made available to the requesting job\n\nHowever, as we noted, these resources aren’t freely floating, but are tied to specific nodes. Therefore, when specifying HPC resources, we need to do so at the node level.\nFor example, the character string select=1:ncpus=8:mem=16GB\nspecifies 1 node (select=1), with 8 CPUs (ncpus=8) and 16 GB of RAM (mem=16GB).\nThe string select=3:ncpus=2:mem=6GB\nspecifies 3 nodes (select=3), each with 2 CPUs (ncpus=2) and 6 GB of RAM (mem=6GB).\nThe string walltime=01:30:00 specifies 1 hour and 30 minutes of elapsed time as measured by a clock (on the wall!). The time string is specified as HH:MM:SS, for hours:minutes:seconds.\n #### Your Project is your key\nCluster resources used are ‘billed’ to your project. Although use of Artemis is free for USyd researchers, your usage will be tallied against your RDMP, which defines your project.1 In order to maintain fair access to all researchers, a count of recent resource usage by each project is maintained, and those projects with lower recent usage will be given a small amount of priority in the scheduling queue.\nThis recent tally is called your ‘fair share weighting’. You can’t really do anything about it, and you can’t even check your fair share either, so our advice is don’t worry about it!2\n\nYour Project is your key!\nLet’s say this again: Everything that can happen on Artemis is managed by projects.\nYou need an Artemis-enabled project to gain access to Artemis.\nYou need to specify a project when requesting Artemis resources.\nYou can only access the /project and /scratch directories of your project.\nProject!! :smirk: {: .keypoints}\n\n ### Estimating resource requests\nYou might be wondering, “How do I know how much X to request??”. This is a good question, and one without an easy answer. You definitely don’t want to underestimate your job’s requirements, because if you do your job will likely fail and be killed by the scheduler. But you also don’t want to overestimate too much, because then you’ll be requesting huge resources that the scheduler will have trouble finding for you (resulting in longer queue time).\nSome good approaches are: * If you have ever run the code before, use the specs of the computer you ran it on as a starting estimate. Ie, if I know a program runs on my laptop, and takes 2 hours, then I know that my 4 cores and 16GB RAM for 2 hours are enough to run the job to completion. * If you can run your job on only a subset of the data (or runs, or training epochs, etc) then run it on only a couple ‘units’ of your data/runs and see how much resources it uses. You can start by requesting 2 CPUs and 8 GB of RAM for 2 hours. You can then run it on more units, and perhaps even try to estimate a scaling relationship to guess how much running N units of data will require. * If all else fails, start by requesting a small amount, say 2 CPUs and 8 GB of RAM for 2 hours, and keep increasing until your job completes successfully!\n\nUsing multiple CPUs (cores)\nNot all software can use more than 1 CPU core.\nIf requesting multiple cores, make sure your software can use them. Otherwise, they’ll just be sitting idle, and you’ll be waiting longer for resources you won’t use to become available.\nCheck the documentation of the programs you wish to use, and look out for options or flags to enable multi-threading or multi-cpu operation. This is sometimes also called ‘OpenMP’. {: .callout}\n\n\nUsing multiple chunks\nNot all software can spread over multiple chunks.\nIf requesting multiple chunks (with select=n, n>1), make sure your software can use them. Otherwise, they’ll just be sitting idle, and you’ll be waiting longer for resources you won’t use to become available.\nCheck the documentation to see if your program supports distributed computing, usually called MPI (Message Passing Interface). If not, requesting more than one chunk won’t help!\nNote that if you can use multiple chunks, the scheduler will have an easier time finding you, eg, 16 CPUs spread over different chunks, than 1 chunk of 16 CPUs. Something to keep in mind! {: .callout}\n\n\n\nJob queues\nThe Artemis queue is broken down into sub-queues, called job queues, which each have different resources and limits allocated to them. Depending on what resources you wish to use, you will be assigned to a particular queue. In some cases, you’ll need to choose a special queue to use particular resources, such as GPUs. This will be discussed further in the next Episode.\n ### Scheduler ‘PBS’ directives\nWe communicate with the scheduler through the use of ‘PBS directives’, which are simply instructions that we pass to the scheduler when we submit a job, like options passed to a program. These directives are invoked using option flags, eg -P <project_name> for the ‘project name’ directive.\nThe main command that we use to submit jobs to the scheduler queue is qsub. Scheduler directives are passed along as options to the qsub command, eg qsub -P <project_name>.\n\nCommon PBS directives\nThe most common PBS directives are listed below. The only compulsory directive is -P (project). Without specifying a project name, an attempted job submission will fail.\n\n\n\n\n\n\n\n\nFlag\nArgument\nNotes\n\n\n\n\n-P\nProject short name\nRequired directive!\n\n\n-N\nJob name\nName it whatever you like; no spaces\n\n\n-l\nResource request\nUsually a select: or walltime: block\n\n\n-q\nJob queue\nDefaults to defaultQ\n\n\n-M\nEmail address\nNotifications can be sent by email on certain events\n\n\n-m\nEmail options: abe\nReceive notification on (a)bort, (b)egin, or (e)nd of jobs\n\n\n-J\nJob array indices i-j:k\nIntegers; indices will be i to j in steps of k\n\n\n-I\nInteractive mode\nOpens a shell terminal with access to the requested resources\n\n\n-X\nX-Window forwarding; use with -I\nAllows use of GUIs in interactive jobs\n\n\n-W\nFurther arguments\nSee manual page for more info: man qsub\n\n\n\n ## PBS scripting\nThere are a number of ways to pass the PBS directives to the qsub submission command. However, the most flexible and reliable way is to write a submission script. We call these ‘PBS scripts’, after the name of the scheduling software, ‘PBS Pro’.\nA PBS script is simply a shell script: a text file that lists commands to send to your command interpreter ‘shell’.3 On Artemis, the default shell is ‘Bash’ (“Bourne-again shell”); so we’ll also sometimes call these ‘Bash scripts’. A PBS script is a bash script written to be executed by the PBS scheduler’s qsub command – which means it contains PBS directives to tell the scheduler what we want it to do.\nLet’s take a look at a real life PBS script to make all this concrete.\nNavigate to our Training project directory – the /project folder for RDS-CORE-Training-RW:\ncd /project/Training\n{: .language-bash}\nLocate the data archive we’ll be using for this course, in the DATA folder:\nls -lsh DATA\n{: .source}\n[jdar4135@login3 Training]$ ls -lsh DATA/\ntotal 2.5G\n1.7G -rw-r-xr-- 1 nbut3013 RDS-CORE-Training-RW 1.7G Jul  9  2019 Automation.tar.gz\n244M -rw-r--r-- 1 nbut3013 RDS-CORE-Training-RW 244M Feb 14  2019 Sample_data.tar.gz\n 25M -rw-r--r-- 1 nbut3013 RDS-CORE-Training-RW  25M Sep  7  2017 cfa_ref_CanFam3.1_chr5.fa.gz\n244M -rw-r--r-- 1 nbut3013 RDS-CORE-Training-RW 244M Jun 22 17:15 data_hpc.tar.gz\n4.0K -rw-r--r-- 1 nbut3013 RDS-CORE-Training-RW  671 Jul  9  2019 dogScripts.tar.gz\n244M -rw-r--r-- 1 nbut3013 RDS-CORE-Training-RW 244M Feb 14  2019 sample_data.tar.gz\n{: .output}\nThese files are .tar archives, like .zip files you might be more familiar with. They are made and read using the tar command. We’ll be using data_hpc2.tar.gz.\nBefore “untar”ing it, create a working directory for yourself. Since we’ll all be working on the same files, we can’t all do that in /project/Training, as we’d either overwrite each other’s, or get ‘Permission denied’ errors if we tried.\nCreate your own directory with the ‘make directory’ command mkdir, naming it whatever you like, then change into it. I’ll use my name, but you should obviously substitute the directory name you have chosen:\nmkdir hayim\ncd hayim\n{: .language-bash}\nNow untar (decompress) the data archive into your directory:\ntar -xzvf ../DATA/data_hpc2.tar.gz\n{: .language-bash}\n[jdar4135@login3 hayim]$ tar -xvzf ../DATA/data_hpc2.tar.gz\ndatahpc/\ndatahpc/hello.pl\ndatahpc/canfam3_chr5.fasta\ndatahpc/align.pbs\ndatahpc/134_R2.fastq.gz\ndatahpc/index.pbs\n....\n{: .output}\nThe option flags -xzvf mean extract files, use Gzip compression (for .tar.gz), be verbose and print what is being done, and use the archive file specified.\nAs can be seen in the output above, the archive contains a folder ‘sample_data’, and this folder has been recreated in your working directory. (Check this by running ls to list the current directory contents!). For convenience, let’s move (mv) all the files (*) out of this extra folder, and remove it (rmdir). Since I am currently in my hayim working directory, and I want the files here also, I’ll use the ‘here’ shortcut (./) as my destination argument:\nmv datahpc/* ./\nrmdir datahpc\n{: .language-bash}\nFinally, make a copy of the index.pbs file, naming the copy basic.pbs, and open it in your text editor of choice:\ncp index.pbs basic.pbs\nnano basic.pbs\n{: .language-bash}\nor if you prefer to use ‘GEdit’: gedit basic.pbs &.\n\n \n\nThe basic.pbs PBS script.\n\n\n\nThe PBS script basic.pbs is constructed from 3 main sections. Let’s take a look at them. But first, take note of the very first line of the script, #!/bin/bash. This declaration is called the shebang, or hash-bang, because it literally starts with a hash # and an exclamation mark !.\nThis first line actually tells the shell how to ‘interpret’ everything that follows – by indicating what command interpreter the commands are written for, and where its program is stored (in /bin). In our case, basic.pbs is a script written for ‘Bash’, using Bash language and commands.\nAlso note the second line of the script, which contains a comment. This is a message to human readers of the script (such as its author!), and is ignored by your computer. In Bash, comments begin with a # character.\n\n\n\n1. PBS directives declaration\nThis is the first section of our PBS script. Whilst the first line was a directive for the shell, these lines contain directives for the PBS scheduler. Each line here contains a PBS directive, which we discussed earlier.\nEach PBS directive begins with the phrase #PBS.\nAt this point you might be thinking, “Hang on – don’t the shebang and PBS directives all begin with a #, indicating a comment?”. And you’d be right! These are all comments, in the sense that they are not executed commands, but are all ‘read’ by someone. Normal comments are read by humans; the shebang is read by the shell; and the #PBS lines are read by the PBS program – it looks out for these lines, and interprets them as its own directives.\n#PBS -P Training\n#PBS -N Index_YourName\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=00:10:00\n#PBS -q small-express\n{: .language-bash}\nEach of these lines declare a PBS directive, telling the scheduler how we would like it to execute our job. Without them, the scheduler has no way to know what resources our job will need. In this example\n\nThe nominated project has short-name Training\nWe have set a job name Index_YourName\nWe have requested resources of 1 chunk with 1 cpu and 4 GB RAM\nWe have requested resources of 10 minutes wall time\nWe have requested the small-express job queue\n\nOther directives, such as email notifications, have not been set, and are not required.\nOnly the Project -P is actually compulsory – the scheduler will choose defaults for other necessary unset directives. These defaults are:\n\n\n\nDirective\nFlag\nDefault value\n\n\n\n\nCompute\n-l\nselect=1:ncpus=1:mem=4GB\n\n\nWall time\n-l\nwalltime=01:00:00\n\n\nJob queue\n-q\ndefaultQ\n\n\n\n\n\n2. Loading modules\nThe next part of our PBS script is reserved for loading Artemis software modules. Software can be loaded any time before its used, but for clarity and to minimise human error, it’s good practice to place all such loading calls at the top of a script (the lilac section above).\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.6\n{: .language-bash}\nHere we have loaded two bioinformatics programs: bwa and samtools. Note that we have also specified the version of the programs we are asking modules to load for us. This is because software packages sometimes change significantly from version to version, using different syntax to run them, or even different methods internally – which can lead to different results on the same data!\nSpecifying the versions of software you use is key for scientific reproducibility.\n\n\n3. Program commands\nFinally, we are ready to tell the scheduler what we want it to actually do – the programs and commands we wish to run. At this point, it is also helpful to first declare any variables or aliases/abbreviations we’d like to use. Eg, we might not want to type out a long directory path name multiple times, so we can create a Bash variable for that path. In the above example:\nio=/project/Training/YourName\n{: .language-bash}\ncreates a variable ‘io’ which can later be retrieved by using Bash’s **\\(** referencing syntax, as ```\\)io```. This code is the green section of our script:\nio=/project/Training/YourName\n\n# BWA index:\nbwa index -a bwtsw ${io}/canfam3_chr5.fasta\n\n# SAMtools index:\nsamtools faidx ${io}/canfam3_chr5.fasta\n{: .language-bash}\nThis part of your script will always be the most variable, as it depends most on the software and your project. In fact, you could store the ‘directives’ section only as a template script that you base all other scripts on.\nProgram function calls generally follow a stereotypical pattern:\nPROGRAM [SUB-FUNCTION] [OPTIONS] INPUT [OUTPUT]\n{: .source}\nEverything after the program name are called the arguments. The [] bracketed arguments above are more likely to be optional, and [] is the general convention for indicating optional arguments.\nIn the example above, the program bwa has a sub-function index. To this is passed an option flag -a followed by its value bwtsw, and then finally an input file ${io}/canfam3_chr5.fasta, using our directory variable io.\nIn general, program options will be specified by a flag followed by its desired value, though some options that don’t have multiple values will just be invoked by their flag. Options also often have long and short names; eg to specify an output file you may be able to write either --output=filename.ext or simply -o filename.ext.\nFinding help with commands?\nThe full range of functions, options and usages that a program offers can be found by invoking its --help option (assuming you’ve loaded the program with Modules first)\nSometimes more information is provided in the program’s manual page, which can usually be loaded by man PROGRAM, eg man samtools. However, note that not all programs are consistent! Some don’t have the --help command engaged. Its good practice but not essential for software developers. Manual pages and help commands are generally alike.\nTry these commands and notice the output ~~~ samtools –help ~~~ {: .language-bash}\nman samtools\n{: .language-bash}\n\n\nNotes\n1↩︎ To get access to Artemis HPC you must have a valid Unikey and be a member of an RDMP (Research Data Management Plan) with Artemis access. RDMPs are managed in the DashR Researcher Dashboard portal.\n2↩︎ Your fair share count decays with a half life of 2 weeks. The contribution of resource usage to your fair share count depends on the queue your job runs in. More on this in Episode 4.\n3↩︎ A shell is a user interface for communicating with the operating system, ie the computer’s main software layer. Shells can be ‘command-line’ (CLI) or graphical (GUI). It’s called a ‘shell’ because it sits around the operating system’s ‘kernel’, which contains the core programming that actually drives the hardware."
  },
  {
    "objectID": "BKP/03.5-break.html",
    "href": "BKP/03.5-break.html",
    "title": "Break!",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/04-submitting-jobs.html",
    "href": "BKP/04-submitting-jobs.html",
    "title": "Submitting and monitoring Artemis Jobs",
    "section": "",
    "text": "In this episode we practice submitting and monitoring jobs on Artemis. We also explore the different job queues available.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/04-submitting-jobs.html#artemis-job-queues",
    "href": "BKP/04-submitting-jobs.html#artemis-job-queues",
    "title": "Submitting and monitoring Artemis Jobs",
    "section": "Artemis Job Queues",
    "text": "Artemis Job Queues\nArtemis’ scheduler reserves portions of the cluster for different job types, in order to make access fairer for all users. However, the most commonly used queues are not specified in a resource request, but are allocated automatically based on what resources are requested. These primary queues all fall under the defaultQ (which is also the default queue set for any job that does not specify its queue with a -q directive).\nThe queues under defaultQ are small, normal, large, highmem and gpu. Jobs are allocated to them according to the resource limits set for each queue: jobs will only run in a queue whose limits it satisfies. Recall that resources are requested with -l directives. The resource limits for each queue are listed below. Importantly, as soon as GPU resources are requested, jobs are assigned to the GPU queue – there is no other queue with GPU resources.\n\n\n\n\n\n\n\n\n\n\n\nQueue\nInvocation\nMax Walltime\nMax Cores perJob / User / Node\nMemory (GB) perNode / Core\nFair Share Weight\n\n\n\n\n-small-normal-large-highmem-gpu\ndefaultQ\n1 day7 days21 days21 days7 days\n24 / 128 / 2496 / 128 / 32 288 / 288 / 32192 / 192 / 64252 / 252 / 36\n123 / < 20123 / < 20123 / < 20123-6144 / > 20185 / –\n1010105050\n\n\nsmall express\nsmall-express\n12 hours\n4 / 40 / 4\n123 / –\n50\n\n\nscavenger\nscavenger\n2 days\n288 / 288 / 24\n123 / –\n0\n\n\ndata transfer\ndtq\n10 days\n2 / 16 / 2\n16 / –\n0\n\n\ninteractive\nqsub -I\n4 hours\n4 / 4 / 4\n123 / –\n100\n\n\n\nTake note of the maxima for each queue. Note especially the maximum cores per node: if you request more than this number of CPUs in a -l select= directive, your job can never run. The highest limit for CPU cores / node is 64, as this is the number of cores of the largest CPUs available on Artemis.\nEach queue also has a different contribution factor to your Fair Share count. For example, use of small-express will accumulate Fair Share 50 times faster than using the defaultQ.\nThere are also a number of additional queues which are not part of defaultQ. These are:\n\n\n\n\n\n\n\nQueue\nPurpose\n\n\n\n\nsmall-express\nFor quick jobs that require few resources.\n\n\nscavenger\nAllows jobs to use any idle resources available in other people’s allocations; however, your job will be suspended if the allocation owner requests those resources!Suspended scavenger jobs will be killed after 24 hours.\n\n\ndtq\nThis queue is reserved for transferring data into or out of Artemis. Users may not try to perform computation in these queues, and the system generally won’t let you.\n\n\ninteractive\nThis is the queue for interactive jobs. It can only be accessed via a qsub -I command.\n\n\n\n\nAllocations\nScavenger uses idle resources available in allocations. An allocation refers to Artemis resources (ie nodes) which have been assigned to certain research groups for priority use. Allocations can be purchased, won, or granted via the Facilities Access Scheme. Remember, your scavenger jobs will be paused (suspended) if the allocation owner requests those resources; they’ll be killed if they are suspended for longer than 24 hours. This makes scavenger an excellent option if you have many small jobs that you can easily re-run if they happen to be killed; some users get thousands of ‘free’ CPU-hours from scavenging!"
  },
  {
    "objectID": "BKP/04-submitting-jobs.html#submitting-jobs",
    "href": "BKP/04-submitting-jobs.html#submitting-jobs",
    "title": "Submitting and monitoring Artemis Jobs",
    "section": "Submitting Jobs",
    "text": "Submitting Jobs\n\nAdjusting PBS directives\nWe’re now ready to submit a compute job to Artemis. Navigate to the sample data we extracted, and open basic.pbs in your preferred text editor.\nnano basic.pbs\n{: .language-bash}\n\n \n\nThe basic.pbs PBS script.\n\n\n\nWe need to make a few edits before we can submit this script. Can you guess what they are?\n\nChange #1\nSpecify your project.\nUse the -P PBS directive to specify the Training project, using its short name. ~~~ #PBS -P Training ~~~ {: .language-bash} {: .solution}\n\n\nChange #2\nGive your job a name\nUse the -N PBS directive to give your job an easily identifiable name. You might run lots of jobs at the same time, so you want to be able to keep track of them!\n#PBS -N Basic_hayim\n{: .language-bash} Substitute a job name of your choice! {: .solution}\n\n\nChange #3\nTailor your resource requests.\nUse the -l PBS directive to request appropriate compute resources and walltime for your job.\nThis script will not be asked to do much, as it’ll just be a first test, so request just 1 minute of walltime, and the minimum RAM, 1 GB. ~~~ #PBS -l select=1:ncpus=1:mem=1GB #PBS -l walltime=00:01:00 ~~~ {: .language-bash} {: .solution}\n\n\nChange #4\nSpecify a job queue.\nUse the -q PBS directive to send the job to the defaultQ queue. You can also try small-express if you like; whose jobs start sooner? ~~~ #PBS -q defaultQ ~~~ {: .language-bash} {: .solution}\n\n\nOptional: Set up email notification\nSet up email notification for your job.\nUse the -M and -m PBS directive to specify a destination email address, and the events you wish to be notified about. You can receive notifications for when your job (b)egins, (e)nds or (a)borts. ~~~ #PBS -M hayim.dar@sydney.edu.au #PBS -m abe ~~~ {: .language-bash} {: .solution}\n\nTo begin, we’re going to run a very simple ‘test’ job, so delete everything below the directives, from # Load modules onward, and replace with\ncd /project/Training/<YourName>\n\nmkdir New_job\ncp basic.pbs New_job/copy.pbs\nperl hello.pl <YourName>\n{: .language-bash}\nPBS uses your /home/<unikey> directory as your default working directory in which to start all PBS scripts. Since your data is not likely to ever be in your home directory, the first command in any script will probably involve setting or changing to the correct folder.\nThe rest of these commands (i) create a new folder, (ii) make a copy of the basic.pbs file within the new folder, and (iii) run a ‘Perl’ script using the perl programming language and interpreter. The script hello.pl accepts one argument.\nSave this PBS script (on nano Ctrl+o), and exit the editor (on nano Ctrl+x).\n\n\nSubmitting PBS scripts to Artemis\nFinally, submit the PBS script basic.pbs to the Artemis scheduler, using qsub:\nqsub basic.pbs\n{: .language-bash}\n[jdar4135@login3 hayim]$ qsub basic.pbs\n2556851.pbsserver\n{: .output}\nCongratulations, you have now submitted your first Artemis job! :tada::tada:"
  },
  {
    "objectID": "BKP/04-submitting-jobs.html#monitoring-artemis-jobs",
    "href": "BKP/04-submitting-jobs.html#monitoring-artemis-jobs",
    "title": "Submitting and monitoring Artemis Jobs",
    "section": "Monitoring Artemis jobs",
    "text": "Monitoring Artemis jobs\n\nqstat\nYou’ve submitted a job, but how can you check what it’s doing, if it’s finished, and what it’s done?\nNote that when we submitted our PBS script above, we received the feedback\n2557008.pbsserver\n{: .output}\nThis number, XXXXXX.pbsserver is the job ID number for this job, and is how we can track its status.\nWe can query a current Artemis job using the PBS command qstat and the job ID. The flag -x will give us job info for historical jobs, so include it to see our job even if it has already completed\nqstat -x 2557008\n{: .language-bash}\n[jdar4135@login3 hayim]$ qstat -x 2557008\nJob id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n2557008.pbsserver Basic_hayim      jdar4135                 0 Q small\n{: .output}\nqstat shows that my job, 2556851.pbsserver, with the name I gave it, Index_hayim, is currently queued in the small job queue. The job status column S shows whether a job is (Q)ueued, (R)unning, (E)xiting, (H)eld, (F)ininshed, or is a (B)atch array job. More status codes can be found in the docs (man qstat).\nqstat has many other options. Some common ones are:\n\n\n\n\n\n\n\nFlag\nOption\n\n\n\n\n-T\nshow an estimated Start Time for jobs\n\n\n-w\nprint wider output columns, for when your job names are longer than 10 characters\n\n\n-u\nshow jobs for a specific user (Unikey)\n\n\n-x\nshow finished jobs also\n\n\n-f\nshow full job details\n\n\n-Q\nprint out numbers of jobs queued and running in for all of Artemis’ job queues\n\n\n\nPrint out the entire job list, by not specifying a job ID, with estimated start times, by running qstat -T. How far down are our training jobs?\n\n\njobstat\nArtemis provides another tool for checking your jobs, which also shows some extra information about Artemis. This is jobstat:\n[jdar4135@login3 hayim]$ jobstat\n......\nSystem Status --------------------------------------------\nCPU hours for jobs currently executing: 1482286.8\nCPU hours for jobs queued:              489049.4\nStorage Quota Usage ------------------------------------------------\n/home                             jdar4135       5.214G          10G\n/project              RDS-CORE-Training-RW       34.07G           1T\n/project                   RDS-CORE-CLC-RW           4k           1T\n/project                   RDS-CORE-ICT-RW       514.3M           1T\n/project            RDS-CORE-SIHclassic-RW         240k           1T\n/project            RDS-CORE-SIHsandbox-RW       236.7G           1T\nStorage Usage (Filesystems totals) ---------------------------------\nFilesystem Used     Free\n/scratch   378.1Tb  1.5%\n{: .output}\nNeat!\n By this time, our tiny test jobs should have run and competed. Check again\nqstat -x 2556851\n{: .language-bash}\n[jdar4135@login3 hayim]$ qstat -x 2557008\nJob id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n2557008.pbsserver Basic_hayim      jdar4135          00:00:00 F small\n{: .output}\nMy job finished! Has yours? If you requested email notifications, did you get any?\n\n\nPBS log files\nNow, list the contents of your working directory\nls -lsh\n{: .language-bash}\n[jdar4135@login3 hayim]$ ls -lsh\ntotal 271M\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW 116M Nov 30  2016 134_R1.fastq.gz\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW 117M Nov 30  2016 134_R2.fastq.gz\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW  748 Oct 25 11:48 align.pbs\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW  203 Oct 25 14:34 basic.pbs\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW  39M Nov 30  2016 canfam3_chr5.fasta\n......\n{: .output}\nNotice anything new? There should be three new files, all beginning with your job name ~~~ -rw——- 1 jdar4135 RDS-CORE-Training-RW 0 Oct 25 15:35 Index_hayim.e2557008 -rw——- 1 jdar4135 RDS-CORE-Training-RW 31 Oct 25 15:35 Index_hayim.o2557008 -rw-r–r– 1 jdar4135 RDS-CORE-Training-RW 1.3K Oct 25 15:35 Index_hayim.o2557008_usage ~~~ {: .output}\nThese are the log files for your job:\n|:–|—| | JobName.eJobID | Error log: This is where error messages – those usually printed to stderr – are recorded | | JobName.oJobID | Output log: This is where output messages – those usually printed to stdout – are recorded | | JobName.oJobID_usage | Usage report: gives a short summary of the resources used by your job |\nCheck whether there were any errors in your job by inspecting the contents of the error log with cat:\n[jdar4135@login3 hayim]$ cat Index_hayim.e2557008\n{: .output}\nEmpty! That’s a good sign. Now let’s have a look at the output log:\n[jdar4135@login3 hayim]$ cat Index_hayim.o2557008\nHello, world! My name is Hayim.\n{: .output}\nThe output from the hello.pl script appears in the PBS output log as expected.\nFinally, let’s have a look at the resource usage report:\n[jdar4135@login3 hayim]$ cat Index_hayim.o2557008_usage\n-- Job Summary -------------------------------------------------------\nJob Id: 2557008.pbsserver for user jdar4135 in queue small\nJob Name: Index_hayim\nProject: RDS-CORE-Training-RW\nExit Status: 0\nJob run as chunks (hpc016:ncpus=1:mem=1048576kb)\nWalltime requested:   00:01:00 :      Walltime used:   00:00:03\n                               :   walltime percent:       5.0%\n-- Nodes Summary -----------------------------------------------------\n-- node hpc016 summary\n    Cpus requested:          1 :          Cpus Used:    unknown\n          Cpu Time:    unknown :        Cpu percent:    unknown\n     Mem requested:      1.0GB :           Mem used:    unknown\n                               :        Mem percent:    unknown\n\n-- WARNINGS ----------------------------------------------------------\n\n** Low Walltime utilisation.  While this may be normal, it may help to check the\n** following:\n**   Did the job parameters specify more walltime than necessary? Requesting\n**   lower walltime could help your job to start sooner.\n**   Did your analysis complete as expected or did it crash before completing?\n**   Did the application run more quickly than it should have? Is this analysis\n**   the one you intended to run?\n**\n-- End of Job Summary ------------------------------------------------\n{: .output}\nWhat does the report show?\n\nExit status\nAcross *NIX systems and programming generally, an Exit Status code of 0 indicates that a program completed successfully.1 Any exit code above 0 usually indicates an error.\nOne way to check for errors in your jobs then, is to search for the words ‘Exit Status’ in your log files ~~~ grep -se “Exit Status” * ~~~ {: .language-bash} {: .callout}\n\n\n\nDid what we expected happen?\nThe final test of whether a job ran correctly is to check whether the outputs you were expecting to be produced actually were produced. In our case, the script basic.pbs was meant to create a new folder and copy itself into it. We have already seen that hello.pl was run successfuly, so check for the rest:\nOur ls command earlier revealed that the new folder was successfully created: ~~~ drwxr-sr-x 2 jdar4135 RDS-CORE-Training-RW 4.0K Oct 25 15:16 New_job ~~~ {: .output}\nso check inside:\n[jdar4135@login3 hayim]$ ls -lh New_job/\ntotal 512\n-rw-r----- 1 jdar4135 RDS-CORE-Training-RW 203 Oct 25 15:35 copy.pbs\n{: .language-bash}\nSuccess.\n ## Practise makes perfect: Python\nLet’s do that again. This time rather than set submit a script to the scheduler that prints a generic hello world message, we will run python code that performs computation.\nInvestigate the computepi_fire.py python file. Given a number of trials to run, this code estimates the number pi by randomly assigning points within a square and comparing the number of points that fall within a circle of radius one relative to the total number of points.\nThe script that submits this python code to the scheduler is the estimate_pi.pbs script.\nnano estimate_pi.pbs\n{: .output}\nThings to notice in the script:\na, A specific version of python is required. The code uses open source software called python fire that exposes python functions and classes to the command line in an easy manner. This dependency is installed with the pip command, a python specific package manager. Python fire requires a python 3X version.\nb, The cd $PBS_O_WORKDIR command changes the directory to the location where the pbs script was submitted. This is an example of a shell variable that has been set by the pbs environment. More examples of environment variables are given at the end of the material.\nc, The code requires an input parameter representing the number of random points used in the simulation. Increasing this will improve the accuracy at the expense of longer run time. Lets try!\nSubmit this script to the scheduler. Check the log files for errors and the output.\nqsub estimate_pi.pbs\n{: .output}\nAfter this, try increasing the simulation number in the pbs script to one million, and practice deleting a job by using the qdel command with the job number that was printed used as an input argument. Here is an example:\n[TRAINING kmar7637@login4 datahpc]$ qsub estimate_pi.pbs\n463292.pbstraining\n[TRAINING kmar7637@login4 datahpc]$ qdel 463292\nqdel: Job has finished\n{: .output}\nTask: If you look again at the python code, there are optional arguments that could make your code even faster (apart from requesting more resources). Can you guess what they are and how to run it???"
  },
  {
    "objectID": "BKP/04-submitting-jobs.html#practise-makes-perfect-matlab",
    "href": "BKP/04-submitting-jobs.html#practise-makes-perfect-matlab",
    "title": "Submitting and monitoring Artemis Jobs",
    "section": "Practise makes perfect: Matlab",
    "text": "Practise makes perfect: Matlab\nLets try one more job submission to reinforce the habit. This time we will submit a completely different matlab script that performs aggregation calculations on a csv file.\nIf you are familiar with Matlab on your local machine, using Matlab is a little different here on Artemis. High performance computers generally work best when engaging software via Linux commands. Applications that rely on graphical interfaces (while achievable with a Nomachine interface or X11 forwarding described earlier) generally are slow and buggy. We will submit a pbs script that executes a Matlab script in the advisable manner that suppresses graphical interaction.\nBefore we begin, lets investigate the underlying data with bash commands. Working with large data files is covered in other training material - it’s an art in itself - however, we’ll get a feel for what we are working with with the head and wc commands.\nhead airline2008.csv\nwc -l airline2008.csv\n{: .output}\nThe output reveals the csv file contains 1,754 rows of flight information containing columns that give arrival and departure time information.\nThe pbs script airline.pbs runs Matlab data computations on this csv file. A couple of things to notice are:\n\nThe extra flags -nodisplay -nosplash suppress the matlab graphical interaction.\nThe command line way to run Matlab with the -r (run) flag. There are a few ways to do it - consult Matlab documentation for more examples.\n\nnano airline.pbs\n{: .output}\nqsub airline.pbs\n{: .output}\nIf there are no errors, investigate the output file to find out what date incurred the longest flight delays.\n\nNotes\n1↩︎ It’s important to keep in mind that ‘success’ to the operating system is not necessarily the same thing as ‘success’ in generating the results that you wanted from your job. Even with an exit status of zero, you should still check that your job produced the expected output.\n2↩︎ The \\ backslash in this block allow the code to be broken over multiple lines, allowing for neater code that doesn’t train off the screen. There must be a space before the backslashes, and no space between the backslash and ‘enter’."
  },
  {
    "objectID": "BKP/05-quiz-extra.html",
    "href": "BKP/05-quiz-extra.html",
    "title": "Quiz! and Case Study",
    "section": "",
    "text": "Have a go at the questions below BEFORE revealing the answer!!\nBetter yet, wait for the instructor and we’ll go through the quiz all together. :blush:\n\nHow do you submit a job called align.pbs?\n\n\n\nqsub align.pbs\n{: .language-bash} {: .solution}\n\n\nHow do you check the status of the job?\n\n\n\nqstat <JOBID>\nqstat -x -u <UNIKEY>\n{: .language-bash} {: .solution}\n\n\nHow might you know when the job is finished?\n\n\n\nThe output of ‘qstat’ shows an ‘F’ in the ‘S’ (status) column. We receive an email from the system indicating termination. We can see the log files for this job (they are only created at completion). {: .solution}\n\n\nHow much space are you allocated in project, and how do you check how much you have used?\n\n\n\nSpace per project: 1 TB, command ‘pquota’ {: .solution}\n\n\nWhat log files are produced after a job completes, and what do they contain?\n\n\n\nJobName.oJobID–standard output (things that are usually printed to terminal)\nJobName.eJobID–standard error (OS or application error messages)\nJobName.oJobID_usage–job resource usage {: .solution}\n\n\nWhat do the -P,-N, and -q directives indicate to the scheduler?\n\n\n\n#PBS -P yourproject # Specify your project.\n#PBS -N yourjobname #Specify the name of you job.\n#PBS -q defaultQ #Request which queue to run your job on.\n{: .language-bash} {: .solution}\n\n\nWrite the directive requesting 10.5 hours wall time\n\n\n\n#PBS –l walltime=10:30:00\n{: .language-bash} {: .solution}\n\n\nWrite the directive to specify a 6-core job using 2 GB RAM per core\n\n\n\n#PBS –l select=1:ncpus=6:mem=12GB\n{: .language-bash} or ~~~ #PBS –l select=6:ncpus=1:mem=2GB ~~~ {: .language-bash} {: .solution}\n\n\nYour job uses the software ‘beast’. What needs to come before your command to run beast?\n\n\n\nmodule load beast\n{: .language-bash} {: .solution}\n\n\nWhere should all important input and output data be stored long-term and why?\n\n\n\nThe Research Data Store (RDS). RDS is backed up and Artemis is not! {: .solution}\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/05-quiz-extra.html#additional-notes",
    "href": "BKP/05-quiz-extra.html#additional-notes",
    "title": "Quiz! and Case Study",
    "section": "Additional notes",
    "text": "Additional notes\n\nPBS Environment Variables\nShell ‘environment variables’ are variables automatically set for you, or specified in a shell’s configuration. When you invoke a PBS shell environment, for example with a qsub command, certain variables are set for you. Some of the more useful are listed below:\n\n\n\n\n\n\n\n\nPBS variable\nMeaning\nUse\n\n\n\n\nPBS_O_WORKDIR\nIs set to the current working directory from which you ran qsub\ncd $PBS_O_WORKDIR at the beginning of your script will change PBS into the current directory, allowing access to any data stored there\n\n\nNCPUS\nThe number of CPUs requested via -l select=\nTo ensure you tell any programs the correct number of CPUs they have access to, pass $NCPUS instead of a number as the argument\n\n\nPBS_JOBID\nThe JobID assigned to the job\nUse $PBS_JOBID to give a unique name to any output or log files generated\n\n\n\n How might we have used NCPUS in the a command within our pbs jobscript?\n\nAnswer\nUse the $NCPUS variable to specify the number of threads we want, e.g. in the bwa mem program you can use: ~~~ bwa mem -M -t $NCPUS -R ‘@RG:134:illumina:CONNEACXX:MS_134’ ~~~ {: .language-bash} {: .solution}\n\n ### qdel\nIf you find you have submitted a job incorrectly, or simply wish to cancel it for whatever reason, you can use qdel to remove it from the queue, or remove its historical record with -x if it is finished. ~~~ qdel [-x] JobID [JobID …] ~~~ {: .language-bash}\nMore than one JobID may be supplied, separated with spaces.\n ### Log files\nThe log files we have seen so far use Artemis’ default options and naming conventions. You can specify your own log files and names as follows\n#PBS -o OutputFilename\n#PBS -e InputFilename\n{: .language-bash}\nYou can also combine both log files into one, using the join directive; oe combines both into the output log file, and eo combines them in the error file, using the default names unless you specify otherwise. ~~~ #PBS -j oe ~~~ {: .language-bash} ~~~ #PBS -j eo ~~~ {: .language-bash}\nPBS log files are also only created when your job completes. If you want to be able monitor the progress of a program which outputs such information to stdout or stderr, you can pipe these outputs to a file of your choosing, with >. Eg: ~~~ # Program commands myProg -flag option1 inputFile > myLogFile.txt ~~~ {: .language-bash}\nTo redirect both the output (1) and error (2) streams:\n# Program commands\nmyProg -flag option1 inputFile 1> myLogFile_$PBS_JOBID.txt 2> myErrorFile_$PBS_JOBID.txt\n{: .language-bash}\nIf you redirect a message stream via piping to a file, it will no longer be available to PBS, and so the PBS log for that stream will be empty.\nBy default, your log files carry a umask of 077, meaning that no-one else has any permissions to write, execute or even read your logs. If you want other people in your project to be able to read your log files (eg for debugging), then set the umask to 027; if you want everyone to be able to read your log files, set the umask to 022. This is done via the additional_attributes directive ~~~ #PBS -W umask=022 ~~~ {: .language-bash}\n ### Common error exit codes\nAn Exit Status of 0 generally indicates a successfully completed job. Exit statuses up to 128 are the statuses returned by the program itself that was running and failed. Here are a few other common codes to watch out for when your job doesn’t run as expected and you want to know why:\n\n\n\nCode\nMeaning\n\n\n\n\n3\nThe job was killed before it could be run\n\n\n137\nThe job was killed because it ran out of RAM\n\n\n271\nThe job was killed because it ran out of walltime\n\n\n\n ### Still need help? If your solution is not in these notes or on the support pages https://sydneyuni.atlassian.net/wiki/spaces/RC/overview\nThen log a ticket!\nConnect to Sydney University’s Service Managment Portal: https://sydneyuni.service-now.com/sm\nClick on “Submit a Request”\nNavigate to the “High Performance Computing Request”\nHome > Technology > Research services > HPC - High Performance Computing request\nThese requests go straight to the Artemis Service Management team. You can request help installing software, compiling code, and anything else. Also, if you think something weird is happening with Artemis log a ticket (under “High Performance Computing issue”) - these fault reports will help us diagnose what is happening and improve the Artemis service!\n ### Artemis is NOT backed up!\nArtemis is not intended to be used as a data store. Artemis is not backed up, and has limited space. Any data you have finished working with should be transferred to your RCOS space.\nHow to do this is covered in the next course, ‘Data transfer and RDS for HPC’\n\n\n\n\nBack up your data.\n\n\n\n\nCourse survey!\nPlease fill out our course survey before you leave!\nHelp us help you! :smiley: {: .testimonial}"
  },
  {
    "objectID": "BKP/06_biocasestudy.html",
    "href": "BKP/06_biocasestudy.html",
    "title": "Bonus: Bioinformatics Case Study",
    "section": "",
    "text": "What other use cases are ther for HPC?\nCan you run any software?\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "BKP/06_biocasestudy.html#index-a-genome",
    "href": "BKP/06_biocasestudy.html#index-a-genome",
    "title": "Bonus: Bioinformatics Case Study",
    "section": "Index a genome",
    "text": "Index a genome\nOpen index.pbs and make any edits necessary. What will you need to change?\nnano index.pbs\n\n\n\nNano index\n\n\nWhen you’re done, save the script (on nano Ctrl+o), and exit (on nano Ctrl+x).\nSubmit the job as before, with qsub:\nqsub index.pbs\n[jdar4135@login3 hayim]$ qsub index.pbs\n2557043.pbsserver\nThis time I’m going to check on the job using qstat -wTu jdar4135 to show my jobs, in wide format, with a start time estimate – it’s a little wide for this broswer window!\n[jdar4135@login3 hayim]$ qstat -wTu jdar4135\n\npbsserver:\n                                                                                  Est\n                                                                                                   Req'd  Req'd   Start\nJob ID                         Username        Queue           Jobname         SessID   NDS  TSK   Memory Time  S Time\n------------------------------ --------------- --------------- --------------- -------- ---- ----- ------ ----- - -----\n2557043.pbsserver              jdar4135        small-express   Index_hayim        49847    1     1    4gb 00:10 R --\nWhen your job finishes (how will you know?) have a look at any log files produced. What is one way to check if there were any errors?\n\nAnswer\nSearch for Exit Status messages with grep:\ngrep -se \"Exit Status\" *\n[jdar4135@login3 hayim]$ grep -se \"Exit Status\" *\nIndex_hayim.o2557008_usage:Exit Status: 0\nIndex_hayim.o2557043_usage:Exit Status: 0\nBoth of my jobs have Exit Status 0 – perfect.\nHave a look at your error file anyway. What do you notice?\n\nAnswer\nNot every message to stderr is an actual error! Blame the program authors for this…\n\nFinally, have a look at the resource usage for this job\n[jdar4135@login3 hayim]$ cat Index_hayim.o2557043_usage\n-- Job Summary -------------------------------------------------------\nJob Id: 2557043.pbsserver for user jdar4135 in queue small-express\nJob Name: Index_hayim\nProject: RDS-CORE-Training-RW\nExit Status: 0\nJob run as chunks (hpc056:ncpus=1:mem=4194304kb)\nWalltime requested:   00:02:00 :      Walltime used:   00:00:41\n                               :   walltime percent:       34.2%\n-- Nodes Summary -----------------------------------------------------\n-- node hpc056 summary\n    Cpus requested:          1 :          Cpus Used:    unknown\n          Cpu Time:    unknown :        Cpu percent:    unknown\n     Mem requested:      1.0GB :           Mem used:    unknown\n                               :        Mem percent:    unknown\n\n-- WARNINGS ----------------------------------------------------------\n\n** Low Walltime utilisation.  While this may be normal, it may help to check the\n** following:\n**   Did the job parameters specify more walltime than necessary? Requesting\n**   lower walltime could help your job to start sooner.\n**   Did your analysis complete as expected or did it crash before completing?\n**   Did the application run more quickly than it should have? Is this analysis\n**   the one you intended to run?\n**\n-- End of Job Summary ------------------------------------------------\nThis job still did not run long enough for the system to properly estimate CPU or RAM usage."
  },
  {
    "objectID": "BKP/06_biocasestudy.html#align-a-genome",
    "href": "BKP/06_biocasestudy.html#align-a-genome",
    "title": "Bonus: Bioinformatics Case Study",
    "section": "Align a genome",
    "text": "Align a genome\nLast one. Open align.pbs and make any edits necessary. What will you need to change?\nnano align.pbs\n\n\n\nThe align.pbs PBS script\n\n\n\nChange #1\n\nSpecify your project.\nUse the -P PBS directive to specify the Training project, using its short name.\n\n\n<details>\n<summary>Solution</summary>\n\n```sh\n#PBS -P Training\n\n\n\nChange #2\n\nGive your job a name\nUse the -N PBS directive to give your job an easily identifiable name. You might run lots of jobs at the same time, so you want to be able to keep track of them!\n\n\n\nSolution\n\n#PBS -N Align_hayim\nSubstitute a job name of your choice!\n\n\n\nChange #3\n\nTailor your resource requests.\nUse the -l PBS directive to request appropriate compute resources and wall-time for your job.\nThis script performs an ‘alignment’ of DNA reads against a reference genome. 10 minutes should be plenty, and we won’t use more than 1 GB RAM.\nNotice here that we are requesting 2 CPUs. Are we sure our programs can use them?\n\n\n\nSolution\n\n#PBS -l select=1:ncpus=2:mem=8GB\n#PBS -l walltime=00:10:00\n\n\n\nChallenge 4\n\nSpecify a job queue.\nUse the -q PBS directive to send the job to the defaultQ queue. You can also try small-express if you like; whose jobs start sooner?\n\n\n\nSolution\n\n#PBS -q defaultQ\n\n\n\nChallenge N5\nSet the working directory for this job. Either just cd to it, or set up a Bash variable to use within the script.\n# Edit the line beginning ```io=```\nio=/project/Training/hayim\n\n\nSolution\n\nThis is my solution"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Day 2: Counts to genes and pathways",
    "section": "",
    "text": "In part 2 of this workshop series, participants will use raw gene count data generated in part 1 to perform downstream analysis of RNA-seq data using R/Rstudio, including how to analyse raw count data to obtain differentially expressed genes and pathways. We will be working on Pawsey’s Nimbus Cloud platform.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Day 2: Counts to genes and pathways",
    "section": "Target audience",
    "text": "Target audience\nThis workshop series is suitable for people who are familiar with working at command line interface and may be new to RNA-seq for differential expression analysis. The course is beginner friendly and intended for those interested in using the command line interface for their analysis. It is also suitable for those who want to learn about and use nf-co.re workflows."
  },
  {
    "objectID": "index.html#how-to-navigate-the-web-pages",
    "href": "index.html#how-to-navigate-the-web-pages",
    "title": "Day 2: Counts to genes and pathways",
    "section": "How to navigate the web pages",
    "text": "How to navigate the web pages\nPlease use the Menu bar to move between lessons. We will start Day 1 with the Setup page and then naviate to the right. Each main menu header has sub-header which will be visible when you click on the main-menu, as shown below."
  },
  {
    "objectID": "index.html#course-survey",
    "href": "index.html#course-survey",
    "title": "Day 2: Counts to genes and pathways",
    "section": "Course survey!",
    "text": "Course survey!\nPlease fill out our course survey before you leave! Help us help you! 😁"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Day 2: Counts to genes and pathways",
    "section": "Credits",
    "text": "Credits\nThis workshop series developed by the Sydney Informatics Hub, University of Sydney in partnership with the Pawsey Supercomputing Research Centre, AARnet, QCIF, and the Australian BioCommons."
  },
  {
    "objectID": "notebooks/0.0_recap_Day1.html",
    "href": "notebooks/0.0_recap_Day1.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.1_recap_casestudy.html",
    "href": "notebooks/0.1_recap_casestudy.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.2_Learning_objectives.html",
    "href": "notebooks/0.2_Learning_objectives.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.3_nextflow_to_RStudio.html",
    "href": "notebooks/0.3_nextflow_to_RStudio.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.0_exploratory_analysis.html",
    "href": "notebooks/1.0_exploratory_analysis.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html",
    "href": "notebooks/1.1_DE_analysis.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "How do we perform differential expression analysis with DESeq2?\nWhat is the DESeq() function?\nWhy should we visualise our results? How do we do this?\n\n\n\n\nWe are happy with what we have observed in our exploratory analysis and are finally ready to start DE analysis.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#experimental-design",
    "href": "notebooks/1.1_DE_analysis.html#experimental-design",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Experimental design",
    "text": "Experimental design\nIn order for DESeq2 to perform DE, we need to revisit the DESeqDataSet object (dds), this time telling it our experimental design. In our case, this will be the column “condition”, taken from “meta”.\n\n\n\n\n\n\nMore complex designs with DESeq2\n\n\n\nDesign formulas can be much, much more complex! This gives you the power to model and account for other variation (e.g. you could model batch effects using something like ~ condition + batch)\n\n\ndds <- DESeqDataSetFromMatrix(countData = counttable,\n                              colData = meta,\n                              design = ~ condition)\n\nChallenge\nLet’s stop here and take some time to understand dds. In the console, type in the code below:\ndds\nNotice dim - can you tell from this how many genes are are analysing?\ncounts(dds)\nThis extracts the count matrix out of dds\ncolData(dds)\nThis extracts our experimental design metadata out of dds\ndesign(dds)\nThis extracts our design formula out of dds"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#explicitly-set-the-factors-levels",
    "href": "notebooks/1.1_DE_analysis.html#explicitly-set-the-factors-levels",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Explicitly set the factors levels",
    "text": "Explicitly set the factors levels\nWhen we perform differential expression and interpret the results, we want to see what changed in the knockout mice (“treatment”) compared to the wildtype mice (“control”) - not the other way around!\nThe experimental design information in dds is stored as a factor in R (check by running class(dds\\$condition - without the backslash). By default, R will choose a reference level for factors based on alphabetical order. That means, the knockout group is currently our baseline (check by typing in the console: dds$condition, without the backslash).\n\n\n\n\n\n\nMarkdown vs R code writing\n\n\n\nThe backslashes are required to escape the “$” as they are interpreted differently in Markdown vs R.\n\n\nWe will need to explicitly set “Wild” as the baseline level for easier interpretation of results.\n# Set Wild to base level, using relevel\ndds$condition <- relevel(dds$condition, \"Wild\")\n# Check that Wild appears as the first level\ndds$condition"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#differential-expression-de-analysis",
    "href": "notebooks/1.1_DE_analysis.html#differential-expression-de-analysis",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Differential expression (DE) analysis",
    "text": "Differential expression (DE) analysis\nBefore we commence with DE, there are some key concepts that you should know:\n\nEach sample is sequenced to a slightly different depth and this needs to be normalised (if we have 60 million reads for sample A but 30 million for sample B, it does not mean that all of sample A’s genes are expressed twice as high!)\nRNA-Seq data count data is distributed in a heteroskedatic manner - in other words, the amount of variance changes with the mean. Lowly expressed genes tend to have a higher read count variance than highly expressed genes. This violates the assumption of most statistical models which assume homoskedatic data. Therefore, the data needs to be transformed.\nDifferential expression tests are performed for every single gene. If we use a simple P < 0.05 cut-off value, 1,000 genes will be defined as DE by chance for a species with ~20,000 genes (humans and mice). Therefore, we need to reduce and adjust for multiple testing.\n\nAll DE methods account for the above in their own way. In this workshop, we will use and explore DESeq2’s method."
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#the-deseq-function",
    "href": "notebooks/1.1_DE_analysis.html#the-deseq-function",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "The DESeq() function",
    "text": "The DESeq() function\nWe are finally ready to perform DE analysis with DESeq2’s DESeq() function. This performs a number of steps required to perform DE - the console output gives you a clue as to what these steps are doing.\n# Perform DE and store the results back in the dds object\ndds <- DESeq(dds)\n# Save the results to res\nres <- results(dds)\nIn brief, by default, DESeq2 is:\n\nEstimating size factors, required to normalise data. DESEq2 uses the median of ratios method. There are many other normalisation methods, each with their pros and cons.\nTransforming the data by estimating dispersion (DESeq2’s way of quantifying within group variability). DESeq2 uses a negative binomial distribution model.\nPerforming independent filtering to reduce the number of statistical tests to perform. DESeq2 will automatically do this. A common method to do this is by removing lowly expressed genes as these don’t have enough data confidently test for DE (DESeq2 actually recommends this to also reduce the size and memory required by DESeq())"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#inspecting-the-results",
    "href": "notebooks/1.1_DE_analysis.html#inspecting-the-results",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Inspecting the results",
    "text": "Inspecting the results\nGet a summary of results by running the code below:\nsummary(res)\nOrder by the smallest adjusted p value, and have a look at the top 5/bottom 5 DE genes.\nres <- res[order(res$padj), ]\nres\nFrom the above, we can see that DE was performed for KO vs Wild samples for 19,859 genes and 6 columns (6 samples). We then see a table of DE results. The column headers include:\n\nbaseMean: this is an average of the normalized count values, dividing by size factors, taken over all samples. This gives you a general idea of how many reads were detected over all samples present for any one gene.\nlog2FoldChange: This measures the magnitude of differential expression of a gene. A positive value indicates that the KO expression was higher than Wild (remember the fuss about setting factor levels?). This number is on the logarithmic scale to base 2, e.g. log2 fold change of 1.5 means that the gene’s expression is increased by 2^1.5 = 2.82 times.\nlfcSE: this is the standard error of the log2FoldChange estimate\nstat: Wald statistic\np-value: Wald test p-value\npadj: p-value adjusted for multiple testing. This is sometimes referred to as the false discovery rate or FDR. By default, DESeq2 performs this with the Benjamini Hochberg method. Note - DESeq2 will report “NA” (not available) values if multiple testing was not applied for this gene, usually because the counts for these gene were too low or the gene was an extreme outlier."
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#defining-significance",
    "href": "notebooks/1.1_DE_analysis.html#defining-significance",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Defining significance",
    "text": "Defining significance\nDifferentially expressed genes are usually defined by cut-offs for two metrics, which are the adjusted p-value and the fold change value. We commonly see differential expression defined as genes with:\n\nAdjusted p-value of < 0.05 (sometimes < 0.1)\nFold change of 2 (log2 fold change ~ 1)\n\nThis is somewhat arbitrary - we need to have just the right number of differentially expressed genes to perform pathway analysis (around 100 - 3,000 is a general guide). Gene expression should be thought of in a biological context - we care about the “top” most differentially expressed genes."
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#subset-the-data-and-write-out-results",
    "href": "notebooks/1.1_DE_analysis.html#subset-the-data-and-write-out-results",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Subset the data and write out results",
    "text": "Subset the data and write out results\nHere we will use padj < 0.05 as our cut-off value for significance and use these genes for enrichment analysis.\n# Redefine the significance cut-off used for independent filtering (default = 0.1). \n# This should be done if we want to use p adj to a value other than 0.1 \nres_padj0.05 <- results(dds, alpha=0.05)\n\n# Subset the results and write these to an output file\nresSig005_subset<-subset(res_padj0.05, padj < 0.05)\nwrite.table(resSig005_subset, \n            \"res_DeSeq2_FDR0.05_comparison_Wild_vs_KO_FUllMatrix.tab\", \n            sep=\"\\t\", \n            col.names=NA, \n            quote=F)\n\n# Reformat the output results into a data.frame\nresSig005_subset <- data.frame(genes = row.names(resSig005_subset), resSig005_subset)\n\n# We can also order padj-filtered results by log fold change\nresSig005_subset_lfc <- resSig005_subset[order(resSig005_subset$log2FoldChange), ]\n\n# Notice how our summary of results has changed slightly now\nsummary(res_padj0.05)\nNormalised count data can be used for visualisation/other analyses. The code below extracts and prints normalised counts to file.\n# Extract normalized counts from dds\nnormalised_counts <- counts(dds, normalized=TRUE)\n\n# Save normalized counts (tab separated) to file\nwrite.table(normalised_counts, \n            \"normalised_all_samples_DeSeq2_FUllMatrix.tab\", \n            sep=\"\\t\", \n            col.names=NA, \n            quote=F)\nNormalised count data can be used for visualisation/other analyses, so it is also handy to save these results.\nnormalised_counts<-counts(dds,normalized=TRUE)\nwrite.table(normalised_counts, \"normalised_all_samples_DeSeq2_FUllMatrix.tab\", sep=\"\\t\", col.names=NA, quote=F)"
  },
  {
    "objectID": "notebooks/1.1_DE_analysis.html#visualise-the-results",
    "href": "notebooks/1.1_DE_analysis.html#visualise-the-results",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "Visualise the results",
    "text": "Visualise the results\n\nVolcano plot\nThe volcano plot is a scatterplot that shows magnitude of change (fold change, x axis) against statistical significance (p-value, y axis). It provides an overall visual snapshot of the number of up and downregulated genes that are statistically significant.\n# Create a basic volcano plot (scatter plot) with x axis = LogFC, y axis = -log10(pvalue)\nresdata <- as.data.frame(res)\n\n# Define whether genes are significantly DE or not and store this in a new column called DE\nresdata$Significant <- \"No\"\nresdata$Significant[resdata$log2FoldChange > 1 & resdata$pvalue < 0.05 ] <- \"Upregulated\"\nresdata$Significant[resdata$log2FoldChange < -1 & resdata$pvalue < 0.05 ] <- \"Downregulated\"\n\n# Create the volcano plot\np <- ggplot(data=resdata,\n       aes(x=log2FoldChange, y=-log10(pvalue), col=Significant)) + geom_point()\n\n# Add significance lines at log2FoldChange -1, 1 and pvalue 0.05\np2 <- p + geom_vline(xintercept=c(-1, 1), col=\"red\") +\n    geom_hline(yintercept=-log10(0.05), col=\"red\")\n\n# Print the plot\np2\n\n\nVisualise some DE genes\nWe have applied low read-count filtering followed by appropriate statistical tests using the DESeq2 package for identification of the differentially expressed geens across our conditions of interest.\nHowever we recommend that you visualise a few genes (of specific interest or otherwise) to check if the identification of these genes is supported by sufficient read-counts.\nUse plotCounts function to plot normalized counts for a single gene of interest. Here we plot\nplotCounts(dds, \n           gene=\"Dip2b\", \n           intgroup=\"condition\")\n\n\nChallenge\nChoose a significant gene that is downregulated in the knockout mice. Enter the plotCounts code in the grey box below to plot the normalized counts for each sample for the gene you have chosen.\n# Code to be removed\nplotCounts(dds, \n           gene=\"Krt2\", \n           intgroup=\"condition\")\n\n\nDiagnostic plots\nBefore we get too excited about our results, we need to confirm that DESeq2’s assumptions were met and that statistical analysis was performed appropriately. We will explore a few plots and concepts to better understand what is happening under the hood.\n\nMA plot\nThe MA plot provides an overview of the relationship between significantly differentially expressed genes, gene expression and log fold change in the form of a scatter plot. Each dot on the plot represents a single gene, significant genes are coloured as a blue dot. The average gene expression is on the x axis (expressed as a mean of normalized counts) and the log fold change is on the Y axis.\n# There is another R function called plotMA, so we need to specify to use DESeq2's plotMA\nDESeq2::plotMA(res, ylim = c(-12, 12))\nThere are a few things we notice:\n\nGenes with a lower mean expression have higher variable log fold changes (heteroskedatic - as we expected)\nThat gene expression is symmetrical around log fold change 0\n\n\n\nDispersion estimates\nThe dispersion plot is useful to examine whether your data is meeting DESeq2’s assumptions around heteroskedasticity and that the data fits DESeq2’s model well. Dispersion is how DESeq2 quantifies variability in the data. It considers variance and mean expression within each experimental group.\nLet’s use plotDispEsts() to generate the dispersion plot and discuss what this means.\n# Plot dispersion estimates using dds\n# Note - we have set our experimental design to ~ condition and it is using this to estimate dispersion\nplotDispEsts( dds)\nThere are a few things to note:\n\nDispersion is higher for genes with small mean of normalized counts, and lower for genes with high mean of normalized counts.\n\nIf you see any other trend, this is a sign that you should not trust DESeq2’s results and that you need to investigate further\n\nTo transform the data, we need to use the variation observed within each experimental group. We cannot do this accurately with few biological replicates (e.g n =3 for KO, n = 3 for wildtype).\n\nDESeq2 assumes that genes with similar expression have a similar level of dispersion to get a more accurate estimation of variability - one of its many benefits! A model (the red curve) is calculated by DESeq2 with this information, using a method called shrinkage.\nIn other words, the red line represents the expected dispersion for any given level of expression\n\nThe black dots represent each gene and their own dispersion (using within group variance as described above)\nThe gene-wise dispersion estimate (black dots) need to be shrunken towards the red line. This helps to reduce false positive results in our differential expression analysis\n\nThere is a lot happening here, but the main point is that our dispersion plot looks as expected and plots should generally appear like this. Check this website for a deeper explanation of this concept, and for examples of what bad dispersion plots look like.\n\n\nHistogram of P-values\nRemember that for every gene, we perform a statistical test to determine whether gene expression is significantly different in the knockout samples, compared to the wildtype. This results in thousands (~20,000 genes in the mouse genome) of p-values. We can look at the histogram of p-values to see how our well our statistical test behaves before we apply correction for multiple testing.\n# Bin frequency of p-value counts by 0.05 incremets (i.e plot 20 columns from p-value of 0 to 1)\nhist(res$pvalue, \n     breaks = 20, \n     col = \"grey\")\nA nice histogram of p-values will have a peak at the 0.05 end, and a uniform frequency at all other p-value bins. Think back to your null and alternate hypothesis. Under the null hypothesis, there is a 5% chance of genes will fall under p-value 0.05, 10 % for p-value under 0.1, etc. The high peak at the first bin (p-value 0 - 0.5) represents genes that reject the null hypothesis (in addition to all the false discoveries - hence our need to adjust for multiple testing!).\nA histogram of p-values that looks anything other than what is described above means that something weird has happened and you may need to contact your local statistician/bioinformatician.\nThis blog post has a nice explanation of each scenario if you want to explore this further.\n\n\n\nKey points\n\nWe need to normalise our DE data to account for differences in sequencing depth between samples\nWe need to transform our data to make sure we don’t violate the assumptions of statistical models we’re applying.\nWe need to account for multiple testing, as we are performing a DE test for every gene.\n\nGene expression should be thought of in its biological context, consider this when defining your significance threshold.\nVisualising our results can aid our interpretation and confirm the statistical assumptions have been met.\n\nThe object class used by the DESeq2 package to store the read counts and the intermediate estimated quantities during statistical analysis is the DESeqDataSet."
  },
  {
    "objectID": "notebooks/1.2_Whatis_Enrichment_analysis.html",
    "href": "notebooks/1.2_Whatis_Enrichment_analysis.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_Day2_wrapup.html",
    "href": "notebooks/2.0_Day2_wrapup.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/3-LoadCountMatrix.html",
    "href": "notebooks/3-LoadCountMatrix.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/4-DE_analysis_2.html",
    "href": "notebooks/4-DE_analysis_2.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/7_R_DEGenes_to_functional_enrichments_clusProf.html",
    "href": "notebooks/7_R_DEGenes_to_functional_enrichments_clusProf.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "RNA-Seq using Nextflow: Day 2",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  }
]